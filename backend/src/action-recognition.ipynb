{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4849320,"sourceType":"datasetVersion","datasetId":2807884}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install opencv-python-headless scikit-learn -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:52:55.774257Z","iopub.execute_input":"2026-01-04T14:52:55.774495Z","iopub.status.idle":"2026-01-04T14:53:00.925018Z","shell.execute_reply.started":"2026-01-04T14:52:55.774473Z","shell.execute_reply":"2026-01-04T14:53:00.924287Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"#  Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport pickle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:00.927068Z","iopub.execute_input":"2026-01-04T14:53:00.927358Z","iopub.status.idle":"2026-01-04T14:53:18.576989Z","shell.execute_reply.started":"2026-01-04T14:53:00.927332Z","shell.execute_reply":"2026-01-04T14:53:18.576421Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"CONFIG = {\n    'SEQUENCE_LENGTH': 20,\n    'IMG_HEIGHT': 112,\n    'IMG_WIDTH': 112,\n    'BATCH_SIZE': 16,\n    'EPOCHS': 12,\n    'LEARNING_RATE': 0.003,\n    'LSTM_HIDDEN': 256,\n    'LSTM_LAYERS': 2,\n    'DROPOUT': 0.3,\n    'NUM_WORKERS': 0\n}\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Available GPUs: {torch.cuda.device_count()}\")\n\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:18.577815Z","iopub.execute_input":"2026-01-04T14:53:18.578223Z","iopub.status.idle":"2026-01-04T14:53:18.699526Z","shell.execute_reply.started":"2026-01-04T14:53:18.578189Z","shell.execute_reply":"2026-01-04T14:53:18.698738Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nAvailable GPUs: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Add Dataset as Input","metadata":{}},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/ucf101-action-recognition/train'\n\nif not os.path.exists(DATA_PATH):\n    print(\"ERROR: Dataset not found\")\nelse:\n    print(f\"Dataset found at: {DATA_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:18.700507Z","iopub.execute_input":"2026-01-04T14:53:18.700900Z","iopub.status.idle":"2026-01-04T14:53:18.712166Z","shell.execute_reply.started":"2026-01-04T14:53:18.700868Z","shell.execute_reply":"2026-01-04T14:53:18.711558Z"}},"outputs":[{"name":"stdout","text":"Dataset found at: /kaggle/input/ucf101-action-recognition/train\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Video Processing Functions","metadata":{}},{"cell_type":"code","source":"def extract_frames(video_path, max_frames=20):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    if total_frames == 0:\n        cap.release()\n        return None\n    \n    step = max(1, total_frames // max_frames)\n    \n    for i in range(0, total_frames, step):\n        if len(frames) >= max_frames:\n            break\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (CONFIG['IMG_WIDTH'], CONFIG['IMG_HEIGHT']))\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n    \n    cap.release()\n    \n    while len(frames) < max_frames:\n        frames.append(frames[-1])\n    \n    return np.array(frames)\n\ndef normalize_frames(frames):\n    return frames / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:18.713001Z","iopub.execute_input":"2026-01-04T14:53:18.713278Z","iopub.status.idle":"2026-01-04T14:53:18.721156Z","shell.execute_reply.started":"2026-01-04T14:53:18.713257Z","shell.execute_reply":"2026-01-04T14:53:18.720573Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Prepare Dataset","metadata":{}},{"cell_type":"code","source":"train_csv = pd.read_csv('/kaggle/input/ucf101-action-recognition/train.csv')\nval_csv = pd.read_csv('/kaggle/input/ucf101-action-recognition/val.csv')\n\ntrain_df = pd.concat([train_csv, val_csv], ignore_index=True)\n\nvideo_paths = []\nlabels = []\n\nbase_path = '/kaggle/input/ucf101-action-recognition'\n\nfor idx, row in train_df.iterrows():\n    video_path = base_path + row['clip_path']\n    if os.path.exists(video_path):\n        video_paths.append(video_path)\n        labels.append(row['label'])\n\nprint(f\"Total videos: {len(video_paths)}\")\nprint(f\"Total classes: {len(set(labels))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:18.722061Z","iopub.execute_input":"2026-01-04T14:53:18.722324Z","iopub.status.idle":"2026-01-04T14:53:55.619759Z","shell.execute_reply.started":"2026-01-04T14:53:18.722298Z","shell.execute_reply":"2026-01-04T14:53:55.619114Z"}},"outputs":[{"name":"stdout","text":"Total videos: 11728\nTotal classes: 101\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Encode Labels","metadata":{}},{"cell_type":"code","source":"selected_classes = ['Basketball', 'Bowling', 'GolfSwing', 'TennisSwing', \n                    'Drumming', 'PushUps', 'PullUps', 'JumpingJack']\n\nfiltered_videos = []\nfiltered_labels = []\n\nfor vid, lbl in zip(video_paths, labels):\n    if lbl in selected_classes:\n        filtered_videos.append(vid)\n        filtered_labels.append(lbl)\n\nvideo_paths = filtered_videos\nlabels = filtered_labels\n\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(labels)\nnum_classes = len(label_encoder.classes_)\n\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Total videos: {len(video_paths)}\")\nprint(f\"Classes: {list(label_encoder.classes_)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:55.621859Z","iopub.execute_input":"2026-01-04T14:53:55.622095Z","iopub.status.idle":"2026-01-04T14:53:55.630779Z","shell.execute_reply.started":"2026-01-04T14:53:55.622075Z","shell.execute_reply":"2026-01-04T14:53:55.630327Z"}},"outputs":[{"name":"stdout","text":"Number of classes: 8\nTotal videos: 1055\nClasses: [np.str_('Basketball'), np.str_('Bowling'), np.str_('Drumming'), np.str_('GolfSwing'), np.str_('JumpingJack'), np.str_('PullUps'), np.str_('PushUps'), np.str_('TennisSwing')]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Train-Test Split with Sampling","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    video_paths, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n)\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:55.631522Z","iopub.execute_input":"2026-01-04T14:53:55.631751Z","iopub.status.idle":"2026-01-04T14:53:55.662815Z","shell.execute_reply.started":"2026-01-04T14:53:55.631733Z","shell.execute_reply":"2026-01-04T14:53:55.662258Z"}},"outputs":[{"name":"stdout","text":"Training samples: 844\nTesting samples: 211\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    def __init__(self, video_paths, labels, sequence_length=20):\n        self.video_paths = video_paths\n        self.labels = labels\n        self.sequence_length = sequence_length\n        \n    def __len__(self):\n        return len(self.video_paths)\n    \n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        label = self.labels[idx]\n        \n        frames = extract_frames(video_path, self.sequence_length)\n        \n        if frames is None:\n            frames = np.zeros((self.sequence_length, CONFIG['IMG_HEIGHT'], CONFIG['IMG_WIDTH'], 3))\n        \n        frames = normalize_frames(frames)\n        frames = torch.FloatTensor(frames).permute(0, 3, 1, 2)\n        \n        return frames, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:55.663483Z","iopub.execute_input":"2026-01-04T14:53:55.663706Z","iopub.status.idle":"2026-01-04T14:53:55.668887Z","shell.execute_reply.started":"2026-01-04T14:53:55.663679Z","shell.execute_reply":"2026-01-04T14:53:55.668247Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Create DataLoaders","metadata":{}},{"cell_type":"code","source":"train_dataset = VideoDataset(X_train, y_train, CONFIG['SEQUENCE_LENGTH'])\ntest_dataset = VideoDataset(X_test, y_test, CONFIG['SEQUENCE_LENGTH'])\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=CONFIG['BATCH_SIZE'], \n    shuffle=True, \n    num_workers=CONFIG['NUM_WORKERS'],\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=CONFIG['BATCH_SIZE'], \n    shuffle=False, \n    num_workers=CONFIG['NUM_WORKERS'],\n    pin_memory=True\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Test batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:55.670539Z","iopub.execute_input":"2026-01-04T14:53:55.670742Z","iopub.status.idle":"2026-01-04T14:53:55.692725Z","shell.execute_reply.started":"2026-01-04T14:53:55.670725Z","shell.execute_reply":"2026-01-04T14:53:55.692182Z"}},"outputs":[{"name":"stdout","text":"Train batches: 53\nTest batches: 14\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# CNN-LSTM Model Architecture","metadata":{}},{"cell_type":"code","source":"class CNNLSTMModel(nn.Module):\n    def __init__(self, num_classes, lstm_hidden=512, lstm_layers=2, dropout=0.4):\n        super(CNNLSTMModel, self).__init__()\n        \n        resnet = models.resnet50(pretrained=True)\n        self.cnn = nn.Sequential(*list(resnet.children())[:-1])\n        \n        for param in list(self.cnn.parameters())[:-20]:\n            param.requires_grad = False\n        \n        self.lstm = nn.LSTM(\n            input_size=2048,\n            hidden_size=lstm_hidden,\n            num_layers=lstm_layers,\n            batch_first=True,\n            dropout=dropout if lstm_layers > 1 else 0,\n            bidirectional=True\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(lstm_hidden * 2, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, num_classes)\n        )\n        \n    def forward(self, x):\n        batch_size, seq_len, c, h, w = x.size()\n        \n        x = x.view(batch_size * seq_len, c, h, w)\n        x = self.cnn(x)\n        x = x.view(batch_size, seq_len, -1)\n        \n        x, _ = self.lstm(x)\n        x = x[:, -1, :]\n        \n        x = self.fc(x)\n        return x\n\nmodel = CNNLSTMModel(num_classes, CONFIG['LSTM_HIDDEN'], CONFIG['LSTM_LAYERS'], CONFIG['DROPOUT'])\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:55.693358Z","iopub.execute_input":"2026-01-04T14:53:55.693618Z","iopub.status.idle":"2026-01-04T14:53:56.833047Z","shell.execute_reply.started":"2026-01-04T14:53:55.693599Z","shell.execute_reply":"2026-01-04T14:53:56.832446Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 191MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Model parameters: 29,941,064\nTrainable parameters: 15,362,312\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Setup Parallel GPU Training","metadata":{}},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'])\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:56.833898Z","iopub.execute_input":"2026-01-04T14:53:56.834174Z","iopub.status.idle":"2026-01-04T14:53:57.196874Z","shell.execute_reply.started":"2026-01-04T14:53:56.834141Z","shell.execute_reply":"2026-01-04T14:53:57.196187Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Training Function","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for frames, labels in tqdm(loader, desc=\"Training\"):\n        frames = frames.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        \n        optimizer.zero_grad()\n        outputs = model(frames)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    return running_loss / len(loader), 100. * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:57.197767Z","iopub.execute_input":"2026-01-04T14:53:57.198040Z","iopub.status.idle":"2026-01-04T14:53:57.203528Z","shell.execute_reply.started":"2026-01-04T14:53:57.198018Z","shell.execute_reply":"2026-01-04T14:53:57.202923Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"#  Validation Function","metadata":{}},{"cell_type":"code","source":"def validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for frames, labels in tqdm(loader, desc=\"Validation\"):\n            frames = frames.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            outputs = model(frames)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return running_loss / len(loader), 100. * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:57.204689Z","iopub.execute_input":"2026-01-04T14:53:57.205220Z","iopub.status.idle":"2026-01-04T14:53:57.225067Z","shell.execute_reply.started":"2026-01-04T14:53:57.205200Z","shell.execute_reply":"2026-01-04T14:53:57.224517Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"best_acc = 0.0\n\nfor epoch in range(CONFIG['EPOCHS']):\n    print(f\"\\nEpoch {epoch+1}/{CONFIG['EPOCHS']}\")\n    \n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n    val_loss, val_acc = validate(model, test_loader, criterion, DEVICE)\n    \n    scheduler.step(val_loss)\n    \n    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n    \n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_acc': val_acc,\n            'label_encoder': label_encoder\n        }, 'best_model.pth')\n        print(f\"Model saved with accuracy: {val_acc:.2f}%\")\n\nprint(f\"\\nBest Validation Accuracy: {best_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:53:57.225839Z","iopub.execute_input":"2026-01-04T14:53:57.226046Z","iopub.status.idle":"2026-01-04T15:48:56.807705Z","shell.execute_reply.started":"2026-01-04T14:53:57.226024Z","shell.execute_reply":"2026-01-04T15:48:56.806827Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:45<00:00,  4.25s/it]\nValidation: 100%|██████████| 14/14 [00:56<00:00,  4.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6370 | Train Acc: 39.34%\nVal Loss: 1.2734 | Val Acc: 45.50%\nModel saved with accuracy: 45.50%\n\nEpoch 2/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:39<00:00,  4.15s/it]\nValidation: 100%|██████████| 14/14 [00:54<00:00,  3.87s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3722 | Train Acc: 52.25%\nVal Loss: 0.9737 | Val Acc: 57.82%\nModel saved with accuracy: 57.82%\n\nEpoch 3/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:39<00:00,  4.15s/it]\nValidation: 100%|██████████| 14/14 [00:54<00:00,  3.86s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.1767 | Train Acc: 58.53%\nVal Loss: 0.8075 | Val Acc: 69.67%\nModel saved with accuracy: 69.67%\n\nEpoch 4/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:39<00:00,  4.14s/it]\nValidation: 100%|██████████| 14/14 [00:53<00:00,  3.85s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.1015 | Train Acc: 59.60%\nVal Loss: 0.6184 | Val Acc: 79.15%\nModel saved with accuracy: 79.15%\n\nEpoch 5/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:39<00:00,  4.14s/it]\nValidation: 100%|██████████| 14/14 [00:53<00:00,  3.86s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0008 | Train Acc: 64.45%\nVal Loss: 0.6814 | Val Acc: 72.51%\n\nEpoch 6/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:39<00:00,  4.14s/it]\nValidation: 100%|██████████| 14/14 [00:54<00:00,  3.86s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9328 | Train Acc: 65.76%\nVal Loss: 0.7495 | Val Acc: 67.30%\n\nEpoch 7/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:39<00:00,  4.14s/it]\nValidation: 100%|██████████| 14/14 [00:54<00:00,  3.86s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9276 | Train Acc: 65.88%\nVal Loss: 0.5964 | Val Acc: 76.30%\n\nEpoch 8/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:41<00:00,  4.19s/it]\nValidation: 100%|██████████| 14/14 [00:54<00:00,  3.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8631 | Train Acc: 67.42%\nVal Loss: 0.5113 | Val Acc: 78.20%\n\nEpoch 9/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:40<00:00,  4.17s/it]\nValidation: 100%|██████████| 14/14 [00:54<00:00,  3.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7646 | Train Acc: 70.26%\nVal Loss: 0.4387 | Val Acc: 81.04%\nModel saved with accuracy: 81.04%\n\nEpoch 10/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:39<00:00,  4.15s/it]\nValidation: 100%|██████████| 14/14 [00:53<00:00,  3.85s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7554 | Train Acc: 71.56%\nVal Loss: 0.7716 | Val Acc: 75.36%\n\nEpoch 11/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:39<00:00,  4.15s/it]\nValidation: 100%|██████████| 14/14 [00:54<00:00,  3.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8450 | Train Acc: 69.31%\nVal Loss: 0.5047 | Val Acc: 78.20%\n\nEpoch 12/12\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 53/53 [03:40<00:00,  4.16s/it]\nValidation: 100%|██████████| 14/14 [00:54<00:00,  3.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7186 | Train Acc: 74.53%\nVal Loss: 0.3971 | Val Acc: 84.83%\nModel saved with accuracy: 84.83%\n\nBest Validation Accuracy: 84.83%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Save Final Model","metadata":{}},{"cell_type":"code","source":"torch.save({\n    'model_state_dict': model.state_dict(),\n    'label_encoder': label_encoder,\n    'config': CONFIG,\n    'num_classes': num_classes\n}, 'action_recognition_model.pth')\n\nwith open('label_encoder.pkl', 'wb') as f:\n    pickle.dump(label_encoder, f)\n\nprint(\"Model saved as 'action_recognition_model.pth'\")\nprint(\"Label encoder saved as 'label_encoder.pkl'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T15:48:56.809121Z","iopub.execute_input":"2026-01-04T15:48:56.809433Z","iopub.status.idle":"2026-01-04T15:48:56.962537Z","shell.execute_reply.started":"2026-01-04T15:48:56.809404Z","shell.execute_reply":"2026-01-04T15:48:56.961863Z"}},"outputs":[{"name":"stdout","text":"Model saved as 'action_recognition_model.pth'\nLabel encoder saved as 'label_encoder.pkl'\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Test Inference","metadata":{}},{"cell_type":"code","source":"def predict_action(video_path, model, label_encoder, device):\n    model.eval()\n    frames = extract_frames(video_path, CONFIG['SEQUENCE_LENGTH'])\n    \n    if frames is None:\n        return \"Error\", 0.0\n    \n    frames = normalize_frames(frames)\n    frames = torch.FloatTensor(frames).permute(0, 3, 1, 2).unsqueeze(0)\n    frames = frames.to(device)\n    \n    with torch.no_grad():\n        outputs = model(frames)\n        probabilities = torch.softmax(outputs, dim=1)\n        confidence, predicted = probabilities.max(1)\n    \n    action = label_encoder.inverse_transform([predicted.item()])[0]\n    confidence = confidence.item() * 100\n    \n    return action, confidence\n\ntest_video = X_test[0]\naction, confidence = predict_action(test_video, model, label_encoder, DEVICE)\n\nprint(f\"Test Video: {os.path.basename(test_video)}\")\nprint(f\"Predicted: {action}\")\nprint(f\"Confidence: {confidence:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T15:48:56.963406Z","iopub.execute_input":"2026-01-04T15:48:56.963680Z","iopub.status.idle":"2026-01-04T15:48:57.372605Z","shell.execute_reply.started":"2026-01-04T15:48:56.963652Z","shell.execute_reply":"2026-01-04T15:48:57.372012Z"}},"outputs":[{"name":"stdout","text":"Test Video: v_GolfSwing_g02_c02.avi\nPredicted: GolfSwing\nConfidence: 50.49%\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Create & Download Model ZIP","metadata":{}},{"cell_type":"code","source":"import zipfile\nfrom IPython.display import FileLink\n\nzip_filename = 'action_recognition_model.zip'\n\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    zipf.write('action_recognition_model.pth')\n    zipf.write('label_encoder.pkl')\n    if os.path.exists('best_model.pth'):\n        zipf.write('best_model.pth')\n\nprint(f\"Zip created: {zip_filename}\")\nprint(f\"Size: {os.path.getsize(zip_filename) / (1024**2):.2f} MB\")\n\nFileLink(zip_filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T15:48:57.373367Z","iopub.execute_input":"2026-01-04T15:48:57.373693Z","iopub.status.idle":"2026-01-04T15:49:14.426959Z","shell.execute_reply.started":"2026-01-04T15:48:57.373658Z","shell.execute_reply":"2026-01-04T15:49:14.426241Z"}},"outputs":[{"name":"stdout","text":"Zip created: action_recognition_model.zip\nSize: 318.89 MB\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/action_recognition_model.zip","text/html":"<a href='action_recognition_model.zip' target='_blank'>action_recognition_model.zip</a><br>"},"metadata":{}}],"execution_count":18}]}