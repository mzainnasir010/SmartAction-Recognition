{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4849320,"sourceType":"datasetVersion","datasetId":2807884}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install opencv-python-headless scikit-learn -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:45:35.442246Z","iopub.execute_input":"2026-01-04T10:45:35.442564Z","iopub.status.idle":"2026-01-04T10:45:38.521206Z","shell.execute_reply.started":"2026-01-04T10:45:35.442535Z","shell.execute_reply":"2026-01-04T10:45:38.520349Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"#  Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport pickle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:45:38.522666Z","iopub.execute_input":"2026-01-04T10:45:38.522945Z","iopub.status.idle":"2026-01-04T10:45:42.170618Z","shell.execute_reply.started":"2026-01-04T10:45:38.522897Z","shell.execute_reply":"2026-01-04T10:45:42.169962Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"CONFIG = {\n    'SEQUENCE_LENGTH': 20,\n    'IMG_HEIGHT': 112,\n    'IMG_WIDTH': 112,\n    'BATCH_SIZE': 8,\n    'EPOCHS': 10,\n    'LEARNING_RATE': 0.0001,\n    'LSTM_HIDDEN': 256,\n    'LSTM_LAYERS': 1,\n    'DROPOUT': 0.3,\n    'NUM_WORKERS': 0\n}\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Available GPUs: {torch.cuda.device_count()}\")\n\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:45:42.171451Z","iopub.execute_input":"2026-01-04T10:45:42.171810Z","iopub.status.idle":"2026-01-04T10:45:42.259740Z","shell.execute_reply.started":"2026-01-04T10:45:42.171786Z","shell.execute_reply":"2026-01-04T10:45:42.259039Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nAvailable GPUs: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Add Dataset as Input","metadata":{}},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/ucf101-action-recognition/train'\n\nif not os.path.exists(DATA_PATH):\n    print(\"ERROR: Dataset not found\")\nelse:\n    print(f\"Dataset found at: {DATA_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:45:42.261480Z","iopub.execute_input":"2026-01-04T10:45:42.261715Z","iopub.status.idle":"2026-01-04T10:45:42.287844Z","shell.execute_reply.started":"2026-01-04T10:45:42.261694Z","shell.execute_reply":"2026-01-04T10:45:42.287333Z"}},"outputs":[{"name":"stdout","text":"Dataset found at: /kaggle/input/ucf101-action-recognition/train\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Video Processing Functions","metadata":{}},{"cell_type":"code","source":"def extract_frames(video_path, max_frames=20):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    if total_frames == 0:\n        cap.release()\n        return None\n    \n    step = max(1, total_frames // max_frames)\n    \n    for i in range(0, total_frames, step):\n        if len(frames) >= max_frames:\n            break\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (CONFIG['IMG_WIDTH'], CONFIG['IMG_HEIGHT']))\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n    \n    cap.release()\n    \n    while len(frames) < max_frames:\n        frames.append(frames[-1])\n    \n    return np.array(frames)\n\ndef normalize_frames(frames):\n    return frames / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:45:42.288615Z","iopub.execute_input":"2026-01-04T10:45:42.288805Z","iopub.status.idle":"2026-01-04T10:45:42.294711Z","shell.execute_reply.started":"2026-01-04T10:45:42.288788Z","shell.execute_reply":"2026-01-04T10:45:42.293820Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Prepare Dataset","metadata":{}},{"cell_type":"code","source":"train_csv = pd.read_csv('/kaggle/input/ucf101-action-recognition/train.csv')\nval_csv = pd.read_csv('/kaggle/input/ucf101-action-recognition/val.csv')\n\ntrain_df = pd.concat([train_csv, val_csv], ignore_index=True)\n\nvideo_paths = []\nlabels = []\n\nbase_path = '/kaggle/input/ucf101-action-recognition'\n\nfor idx, row in train_df.iterrows():\n    video_path = base_path + row['clip_path']\n    if os.path.exists(video_path):\n        video_paths.append(video_path)\n        labels.append(row['label'])\n\nprint(f\"Total videos: {len(video_paths)}\")\nprint(f\"Total classes: {len(set(labels))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:45:42.295746Z","iopub.execute_input":"2026-01-04T10:45:42.296101Z","iopub.status.idle":"2026-01-04T10:46:03.088712Z","shell.execute_reply.started":"2026-01-04T10:45:42.296073Z","shell.execute_reply":"2026-01-04T10:46:03.087884Z"}},"outputs":[{"name":"stdout","text":"Total videos: 11728\nTotal classes: 101\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Encode Labels","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(labels)\nnum_classes = len(label_encoder.classes_)\n\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Sample classes: {list(label_encoder.classes_[:5])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:46:03.090461Z","iopub.execute_input":"2026-01-04T10:46:03.091156Z","iopub.status.idle":"2026-01-04T10:46:03.101566Z","shell.execute_reply.started":"2026-01-04T10:46:03.091126Z","shell.execute_reply":"2026-01-04T10:46:03.100636Z"}},"outputs":[{"name":"stdout","text":"Number of classes: 101\nSample classes: [np.str_('ApplyEyeMakeup'), np.str_('ApplyLipstick'), np.str_('Archery'), np.str_('BabyCrawling'), np.str_('BalanceBeam')]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Train-Test Split with Sampling","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    video_paths, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n)\n\nsample_train = min(2000, len(X_train))\nsample_test = min(400, len(X_test))\n\nX_train = X_train[:sample_train]\ny_train = y_train[:sample_train]\n\nX_test = X_test[:sample_test]\ny_test = y_test[:sample_test]\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:46:03.102581Z","iopub.execute_input":"2026-01-04T10:46:03.102853Z","iopub.status.idle":"2026-01-04T10:46:03.126347Z","shell.execute_reply.started":"2026-01-04T10:46:03.102832Z","shell.execute_reply":"2026-01-04T10:46:03.125672Z"}},"outputs":[{"name":"stdout","text":"Training samples: 2000\nTesting samples: 400\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    def __init__(self, video_paths, labels, sequence_length=20):\n        self.video_paths = video_paths\n        self.labels = labels\n        self.sequence_length = sequence_length\n        \n    def __len__(self):\n        return len(self.video_paths)\n    \n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        label = self.labels[idx]\n        \n        frames = extract_frames(video_path, self.sequence_length)\n        \n        if frames is None:\n            frames = np.zeros((self.sequence_length, CONFIG['IMG_HEIGHT'], CONFIG['IMG_WIDTH'], 3))\n        \n        frames = normalize_frames(frames)\n        frames = torch.FloatTensor(frames).permute(0, 3, 1, 2)\n        \n        return frames, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:46:03.127497Z","iopub.execute_input":"2026-01-04T10:46:03.127842Z","iopub.status.idle":"2026-01-04T10:46:03.139023Z","shell.execute_reply.started":"2026-01-04T10:46:03.127806Z","shell.execute_reply":"2026-01-04T10:46:03.138477Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Create DataLoaders","metadata":{}},{"cell_type":"code","source":"train_dataset = VideoDataset(X_train, y_train, CONFIG['SEQUENCE_LENGTH'])\ntest_dataset = VideoDataset(X_test, y_test, CONFIG['SEQUENCE_LENGTH'])\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=CONFIG['BATCH_SIZE'], \n    shuffle=True, \n    num_workers=CONFIG['NUM_WORKERS'],\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=CONFIG['BATCH_SIZE'], \n    shuffle=False, \n    num_workers=CONFIG['NUM_WORKERS'],\n    pin_memory=True\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Test batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:46:03.141161Z","iopub.execute_input":"2026-01-04T10:46:03.141769Z","iopub.status.idle":"2026-01-04T10:46:03.165852Z","shell.execute_reply.started":"2026-01-04T10:46:03.141740Z","shell.execute_reply":"2026-01-04T10:46:03.165339Z"}},"outputs":[{"name":"stdout","text":"Train batches: 250\nTest batches: 50\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# CNN-LSTM Model Architecture","metadata":{}},{"cell_type":"code","source":"class CNNLSTMModel(nn.Module):\n    def __init__(self, num_classes, lstm_hidden=256, lstm_layers=1, dropout=0.3):\n        super(CNNLSTMModel, self).__init__()\n        \n        resnet = models.resnet50(pretrained=True)\n        self.cnn = nn.Sequential(*list(resnet.children())[:-1])\n        \n        for param in self.cnn.parameters():\n            param.requires_grad = False\n        \n        self.lstm = nn.LSTM(\n            input_size=2048,\n            hidden_size=lstm_hidden,\n            num_layers=lstm_layers,\n            batch_first=True,\n            dropout=dropout if lstm_layers > 1 else 0,\n            bidirectional=True\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(lstm_hidden * 2, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, num_classes)\n        )\n        \n    def forward(self, x):\n        batch_size, seq_len, c, h, w = x.size()\n        \n        x = x.view(batch_size * seq_len, c, h, w)\n        x = self.cnn(x)\n        x = x.view(batch_size, seq_len, -1)\n        \n        x, _ = self.lstm(x)\n        x = x[:, -1, :]\n        \n        x = self.fc(x)\n        return x\n\nmodel = CNNLSTMModel(num_classes, CONFIG['LSTM_HIDDEN'], CONFIG['LSTM_LAYERS'], CONFIG['DROPOUT'])\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:46:03.166611Z","iopub.execute_input":"2026-01-04T10:46:03.166821Z","iopub.status.idle":"2026-01-04T10:46:04.323439Z","shell.execute_reply.started":"2026-01-04T10:46:03.166794Z","shell.execute_reply":"2026-01-04T10:46:04.322690Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 183MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Model parameters: 28,309,413\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Setup Parallel GPU Training","metadata":{}},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'])\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:46:04.324348Z","iopub.execute_input":"2026-01-04T10:46:04.324556Z","iopub.status.idle":"2026-01-04T10:46:04.570215Z","shell.execute_reply.started":"2026-01-04T10:46:04.324535Z","shell.execute_reply":"2026-01-04T10:46:04.569605Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Training Function","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for frames, labels in tqdm(loader, desc=\"Training\"):\n        frames = frames.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        \n        optimizer.zero_grad()\n        outputs = model(frames)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    return running_loss / len(loader), 100. * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:46:04.571057Z","iopub.execute_input":"2026-01-04T10:46:04.571317Z","iopub.status.idle":"2026-01-04T10:46:04.576628Z","shell.execute_reply.started":"2026-01-04T10:46:04.571285Z","shell.execute_reply":"2026-01-04T10:46:04.575919Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"#  Validation Function","metadata":{}},{"cell_type":"code","source":"def validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for frames, labels in tqdm(loader, desc=\"Validation\"):\n            frames = frames.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            outputs = model(frames)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return running_loss / len(loader), 100. * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:46:04.577491Z","iopub.execute_input":"2026-01-04T10:46:04.577745Z","iopub.status.idle":"2026-01-04T10:46:04.593554Z","shell.execute_reply.started":"2026-01-04T10:46:04.577715Z","shell.execute_reply":"2026-01-04T10:46:04.593079Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"best_acc = 0.0\n\nfor epoch in range(CONFIG['EPOCHS']):\n    print(f\"\\nEpoch {epoch+1}/{CONFIG['EPOCHS']}\")\n    \n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n    val_loss, val_acc = validate(model, test_loader, criterion, DEVICE)\n    \n    scheduler.step(val_loss)\n    \n    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n    \n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_acc': val_acc,\n            'label_encoder': label_encoder\n        }, 'best_model.pth')\n        print(f\"Model saved with accuracy: {val_acc:.2f}%\")\n\nprint(f\"\\nBest Validation Accuracy: {best_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:46:04.594348Z","iopub.execute_input":"2026-01-04T10:46:04.594551Z","iopub.status.idle":"2026-01-04T12:55:11.528020Z","shell.execute_reply.started":"2026-01-04T10:46:04.594533Z","shell.execute_reply":"2026-01-04T12:55:11.527274Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:41<00:00,  2.56s/it]\nValidation: 100%|██████████| 50/50 [02:06<00:00,  2.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.6125 | Train Acc: 1.35%\nVal Loss: 4.5858 | Val Acc: 3.25%\nModel saved with accuracy: 3.25%\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:44<00:00,  2.58s/it]\nValidation: 100%|██████████| 50/50 [02:07<00:00,  2.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.5809 | Train Acc: 2.40%\nVal Loss: 4.5621 | Val Acc: 2.75%\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:45<00:00,  2.58s/it]\nValidation: 100%|██████████| 50/50 [02:06<00:00,  2.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.5338 | Train Acc: 4.10%\nVal Loss: 4.4776 | Val Acc: 7.00%\nModel saved with accuracy: 7.00%\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:44<00:00,  2.58s/it]\nValidation: 100%|██████████| 50/50 [02:06<00:00,  2.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.4560 | Train Acc: 4.95%\nVal Loss: 4.3582 | Val Acc: 10.00%\nModel saved with accuracy: 10.00%\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:52<00:00,  2.61s/it]\nValidation: 100%|██████████| 50/50 [02:07<00:00,  2.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.3455 | Train Acc: 7.05%\nVal Loss: 4.2204 | Val Acc: 12.75%\nModel saved with accuracy: 12.75%\n\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:47<00:00,  2.59s/it]\nValidation: 100%|██████████| 50/50 [02:07<00:00,  2.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.2146 | Train Acc: 8.85%\nVal Loss: 4.0549 | Val Acc: 12.75%\n\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:45<00:00,  2.58s/it]\nValidation: 100%|██████████| 50/50 [02:07<00:00,  2.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.0884 | Train Acc: 9.95%\nVal Loss: 3.8879 | Val Acc: 13.50%\nModel saved with accuracy: 13.50%\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:48<00:00,  2.60s/it]\nValidation: 100%|██████████| 50/50 [02:07<00:00,  2.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.9524 | Train Acc: 11.55%\nVal Loss: 3.7785 | Val Acc: 15.50%\nModel saved with accuracy: 15.50%\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:49<00:00,  2.60s/it]\nValidation: 100%|██████████| 50/50 [02:08<00:00,  2.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.8588 | Train Acc: 12.80%\nVal Loss: 3.6282 | Val Acc: 18.25%\nModel saved with accuracy: 18.25%\n\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 250/250 [10:51<00:00,  2.60s/it]\nValidation: 100%|██████████| 50/50 [02:07<00:00,  2.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.7570 | Train Acc: 13.75%\nVal Loss: 3.5467 | Val Acc: 19.00%\nModel saved with accuracy: 19.00%\n\nBest Validation Accuracy: 19.00%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Save Final Model","metadata":{}},{"cell_type":"code","source":"torch.save({\n    'model_state_dict': model.state_dict(),\n    'label_encoder': label_encoder,\n    'config': CONFIG,\n    'num_classes': num_classes\n}, 'action_recognition_model.pth')\n\nwith open('label_encoder.pkl', 'wb') as f:\n    pickle.dump(label_encoder, f)\n\nprint(\"Model saved as 'action_recognition_model.pth'\")\nprint(\"Label encoder saved as 'label_encoder.pkl'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:55:11.529016Z","iopub.execute_input":"2026-01-04T12:55:11.529263Z","iopub.status.idle":"2026-01-04T12:55:11.670766Z","shell.execute_reply.started":"2026-01-04T12:55:11.529232Z","shell.execute_reply":"2026-01-04T12:55:11.670157Z"}},"outputs":[{"name":"stdout","text":"Model saved as 'action_recognition_model.pth'\nLabel encoder saved as 'label_encoder.pkl'\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Test Inference","metadata":{}},{"cell_type":"code","source":"def predict_action(video_path, model, label_encoder, device):\n    model.eval()\n    frames = extract_frames(video_path, CONFIG['SEQUENCE_LENGTH'])\n    \n    if frames is None:\n        return \"Error\", 0.0\n    \n    frames = normalize_frames(frames)\n    frames = torch.FloatTensor(frames).permute(0, 3, 1, 2).unsqueeze(0)\n    frames = frames.to(device)\n    \n    with torch.no_grad():\n        outputs = model(frames)\n        probabilities = torch.softmax(outputs, dim=1)\n        confidence, predicted = probabilities.max(1)\n    \n    action = label_encoder.inverse_transform([predicted.item()])[0]\n    confidence = confidence.item() * 100\n    \n    return action, confidence\n\ntest_video = X_test[0]\naction, confidence = predict_action(test_video, model, label_encoder, DEVICE)\n\nprint(f\"Test Video: {os.path.basename(test_video)}\")\nprint(f\"Predicted: {action}\")\nprint(f\"Confidence: {confidence:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:55:11.672355Z","iopub.execute_input":"2026-01-04T12:55:11.672556Z","iopub.status.idle":"2026-01-04T12:55:12.153912Z","shell.execute_reply.started":"2026-01-04T12:55:11.672538Z","shell.execute_reply":"2026-01-04T12:55:12.153168Z"}},"outputs":[{"name":"stdout","text":"Test Video: v_Basketball_g10_c05.avi\nPredicted: Bowling\nConfidence: 6.75%\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Create & Download Model ZIP","metadata":{}},{"cell_type":"code","source":"import zipfile\nfrom IPython.display import FileLink\n\nzip_filename = 'action_recognition_model.zip'\n\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    zipf.write('action_recognition_model.pth')\n    zipf.write('label_encoder.pkl')\n    if os.path.exists('best_model.pth'):\n        zipf.write('best_model.pth')\n\nprint(f\"Zip created: {zip_filename}\")\nprint(f\"Size: {os.path.getsize(zip_filename) / (1024**2):.2f} MB\")\n\nFileLink(zip_filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:55:12.154770Z","iopub.execute_input":"2026-01-04T12:55:12.155075Z","iopub.status.idle":"2026-01-04T12:55:24.758180Z","shell.execute_reply.started":"2026-01-04T12:55:12.155039Z","shell.execute_reply":"2026-01-04T12:55:24.757553Z"}},"outputs":[{"name":"stdout","text":"Zip created: action_recognition_model.zip\nSize: 228.93 MB\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/action_recognition_model.zip","text/html":"<a href='action_recognition_model.zip' target='_blank'>action_recognition_model.zip</a><br>"},"metadata":{}}],"execution_count":18}]}